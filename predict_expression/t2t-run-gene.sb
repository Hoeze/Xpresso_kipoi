#!/bin/bash

#SBATCH -p gpu
#SBATCH -n 4
#SBATCH --gres=gpu:p100:1
#SBATCH --mem 20000
#SBATCH --time 0-2:0:0
#SBATCH -J predExpr
#SBATCH -o predExpr.out
#SBATCH -e predExpr.err

. ~/tf1.14/bin/activate

# See what problems, models, and hyperparameter sets are available.
# You can easily swap between them (and add new ones).
#t2t-trainer --registry_help

PROBLEM=expression_level_predict_pca
MODEL=transformer_encoder
HPARAMS=transformer_gene #transformer_tiny

USR_DIR=$HOME/predict_expression/
DATA_DIR=$HOME/predict_expression/prepared_data
TMP_DIR=/tmp/t2t_gene/
NUMGPU=1
TRAIN_DIR=$HOME/t2t_train/$PROBLEM/$MODEL-$HPARAMS
STEPS=2000

# echo "Visible devices"
# echo $CUDA_VISIBLE_DEVICES

mkdir -p $DATA_DIR $TMP_DIR $TRAIN_DIR

# # Generate data
# t2t-datagen \
#   --t2t_usr_dir=$USR_DIR \
#   --data_dir=$DATA_DIR \
#   --tmp_dir=$TMP_DIR \
#   --problem=$PROBLEM
#
# # Train
# # *  If you run out of memory, add --hparams='batch_size=1024'.
# t2t-trainer \
#   --t2t_usr_dir=$USR_DIR \
#   --data_dir=$DATA_DIR \
#   --problem=$PROBLEM \
#   --model=$MODEL \
#   --hparams_set=$HPARAMS \
#   --output_dir=$TRAIN_DIR \
#   --train_steps=$STEPS \
#   --worker_gpu=$NUMGPU #\
# #  --hparams=batch_size=1
#
# t2t-pred \
#   --t2t_usr_dir=$USR_DIR \
#   --data_dir=$DATA_DIR \
#   --problem=$PROBLEM \
#   --model=$MODEL \
#   --hparams_set=$HPARAMS \
#   --output_dir=$TRAIN_DIR \
#   --eval_use_test_set=True

python -m tensor2tensor.trax.trainer \
#   --t2t_usr_dir=$USR_DIR \
#   --data_dir=$DATA_DIR \
#   --problem=$PROBLEM \
#   --model=$MODEL \
#   --hparams_set=$HPARAMS \
#   --output_dir=$TRAIN_DIR \
#   --train_steps=$STEPS 

# t2t_avg_all \
#   --model_dir=$TRAIN_DIR \
#   --output_dir=$TRAIN_DIR/avg \
#   --n=1
