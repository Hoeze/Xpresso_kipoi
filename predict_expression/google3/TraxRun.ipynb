{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TraxRun.ipynb","provenance":[],"collapsed_sections":[],"last_runtime":{"build_target":"//learning/deepmind/dm_python:dm_notebook3_tpu","kind":"private"}},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Yhq8iXpcCWNr","colab_type":"text"},"source":["# Setup"]},{"cell_type":"markdown","metadata":{"id":"f1xYaMBqClTO","colab_type":"text"},"source":["The below code mixes TF and JAX, which won't work on stock colab with TPUs.\n","\n","Find the cell of your DM Py3 TPUv2 colab, \n","\n","---\n","\n","\n","```\n","borgcfg --vars=cell=YOURCELL production/borg/colaboratory/mint/deepmind_tpu_py3.borg update colab_kernel.kernel\n","```"]},{"cell_type":"markdown","metadata":{"id":"PVmsh9FDNU_q","colab_type":"text"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"I-CjPIQXFYIb","colab_type":"text"},"source":["# Imports"]},{"cell_type":"code","metadata":{"id":"2KMNDzOUWqme","colab_type":"code","outputId":"60528f0a-61a3-49d3-b73e-e7c6a7a63c5a","executionInfo":{"status":"ok","timestamp":1577910809392,"user_tz":480,"elapsed":27932,"user":{"displayName":"Vikram Agarwal","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB_g3ycHyXK2wQJuaywzxudHa4XAk3F1LNDGJG9=s64","userId":"13055151899643549382"}},"colab":{"height":54}},"source":["from six.moves import cPickle\n","import os\n","import datetime\n","import random\n","import tempfile\n","from functools import partial\n","import copy\n","\n","import gin\n","gin.enter_interactive_mode()\n","\n","import numpy as onp\n","import jax\n","from jax import lax\n","from jax import random as jr\n","from jax import numpy as np\n","from jax.ops import index, index_update\n","\n","from matplotlib import pyplot as plt\n","\n","import tensorflow.google as tf\n","tf.enable_eager_execution()\n","import tensorflow_datasets as tfds\n","\n","from colabtools import adhoc_import\n","with adhoc_import.Google3():\n","  import trax\n","  from trax.supervised import trainer_lib\n","  from trax import layers as tl\n","  from trax.supervised import inputs as trax_input\n","  from trax import models as trax_models\n","  from trax.models import transformer\n","  from trax import optimizers as trax_optimizers\n","  from trax import backend\n","  from trax import shapes"],"execution_count":0,"outputs":[{"output_type":"stream","text":["T2T: skipped importing 1 data_generators modules. OK if no other errors. Depend on _heavy or problem-specific py_binary targets if trying to use a module that was skipped.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"exBUXkX_3bkq","colab_type":"text"},"source":["# Trax Input Fn"]},{"cell_type":"code","metadata":{"id":"RwFhZ7nq1bG5","colab_type":"code","colab":{}},"source":["@gin.configurable()\n","def vikram_inputs(\n","    n_batch=8192,\n","    num_input_timestamps=1001,\n","    num_embed=117,\n","    num_output_predictions=57,\n","    cns_path='/readahead/200M/cns/jq-d/home/levskaya/calico/vikram/',\n","    # shuffle_files=True,\n","    batch_shuffle_size=1024,\n","    n_prefetch=16,\n","    mode='train'):\n","\n","  print(cns_path)\n","  # grab filenames from CNS\n","  train_files = tf.gfile.Glob(os.path.join(cns_path, '*train*'))\n","  dev_files =  tf.gfile.Glob(os.path.join(cns_path, '*dev*'))\n","  test_files = tf.gfile.Glob(os.path.join(cns_path, '*test*'))\n","\n","  # tf.example proto parsing\n","  feature_description = {\n","    \"inputs\": tf.VarLenFeature(tf.float32),\n","    \"targets\": tf.VarLenFeature(tf.float32),\n","  }\n","\n","  def _parse_example(x):\n","    return tf.parse_example([x], feature_description)\n","\n","  # reshaping\n","  input_shape = [-1, num_input_timestamps, num_embed]\n","  input_dtype = np.float32\n","  target_shape = [-1, 1, num_output_predictions]\n","  target_dtype = np.float32\n","  \n","  def _reshape(x):\n","    inps = x['inputs'].values\n","    inps = tf.reshape(inps, input_shape)\n","    #inps = inps[:, 499:501, :] ## LOOK AT PROMOTER REGION ONLY AS BASELINE\n","    tgts = x['targets'].values\n","    # condition = tf.less(tgts, -0.5)\n","    # tgts = tf.dtypes.cast(tf.where(condition, tf.zeros_like(tgts), tf.ones_like(tgts)),tf.int32)\n","    tgts = tf.reshape(tgts, target_shape)\n","    return (inps, tgts)\n","\n","  # tf.data chain\n","  def make_dataset_iterator(data_files):\n","    return (\n","        tf.data.TFRecordDataset(data_files)\n","        .map(_parse_example)\n","        .repeat() #used to be after map_reshape\n","        .shuffle(batch_shuffle_size) #used to be after batch\n","        .batch(n_batch, drop_remainder=True)\n","        .map(_reshape)\n","        .prefetch(n_prefetch)\n","        .as_numpy_iterator()\n","        )\n","\n","  def make_dataset_iterator2(data_files):\n","    return (\n","        tf.data.TFRecordDataset(data_files)\n","        .map(_parse_example) #removed shuffle & repeat operations\n","        .batch(n_batch, drop_remainder=False)\n","        .map(_reshape)\n","        .prefetch(n_prefetch)\n","        .as_numpy_iterator()\n","        )\n","\n","  if mode=='linregtrain':\n","    return make_dataset_iterator2(train_files)\n","  elif mode=='test':\n","    return make_dataset_iterator2(test_files)\n","  else:\n","    # make dataset iterators\n","    ds_train = lambda: make_dataset_iterator(train_files)\n","    ds_dev = lambda: make_dataset_iterator(dev_files)\n","    ds_test = lambda: make_dataset_iterator(test_files)\n","    \n","    # put information in form trax wants\n","    input_shape_without_batch = list(input_shape)[1:]\n","    target_shape_without_batch = list(target_shape)[1:]\n","    return trax_input.Inputs(train_stream=ds_train,\n","                  train_eval_stream=ds_dev,\n","                  eval_stream=ds_test,\n","                  input_shape=input_shape_without_batch,\n","                  input_dtype=input_dtype,\n","                  target_shape=target_shape_without_batch,\n","                  target_dtype=target_dtype)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kcuu0XgAByX_","colab_type":"text"},"source":["# Modified Transformer"]},{"cell_type":"code","metadata":{"id":"_pCxKSIT688r","colab_type":"code","colab":{}},"source":["@tl.base.layer()\n","def no_padding_mask(x, **kwargs):\n","  del kwargs\n","  return np.reshape(np.ones(x.shape[0]*x.shape[-2], dtype=x.dtype), \n","                    (x.shape[0], 1, 1, x.shape[-2]))\n","\n","def _FeedForwardBlock(d_model, d_ff, dropout, layer_idx, mode, activation):\n","  \"\"\"Returns a list of layers implementing a feed-forward block.\n","  Args:\n","    d_model: int:  depth of embedding\n","    d_ff: int: depth of feed-forward layer\n","    dropout: float: dropout rate (how much to drop out)\n","    layer_idx: which layer are we at (for bookkeeping)\n","    mode: str: 'train' or 'eval'\n","    activation: the non-linearity in feed-forward layer\n","  Returns:\n","    A list of layers which maps vectors to vectors.\n","  \"\"\"\n","  dropout_middle = tl.Dropout(\n","      rate=dropout, name='ff_middle_%d' % layer_idx, mode=mode)\n","  dropout_final = tl.Dropout(\n","      rate=dropout, name='ff_final_%d' % layer_idx, mode=mode)\n","\n","  return [\n","      tl.LayerNorm(),\n","      tl.Dense(d_ff),\n","      activation(),\n","      dropout_middle,\n","      tl.Dense(d_model),\n","      dropout_final,\n","  ]\n","\n","def _EncoderBlock(d_model, d_ff, n_heads, dropout, layer_idx, mode,\n","                  ff_activation):\n","  \"\"\"Returns a list of layers that implements a Transformer encoder block.\n","  The input to the layer is a pair, (activations, mask), where the mask was\n","  created from the original source tokens to prevent attending to the padding\n","  part of the input.\n","  Args:\n","    d_model: int:  depth of embedding\n","    d_ff: int: depth of feed-forward layer\n","    n_heads: int: number of attention heads\n","    dropout: float: dropout rate (how much to drop out)\n","    layer_idx: which layer are we at (for bookkeeping)\n","    mode: str: 'train' or 'eval'\n","    ff_activation: the non-linearity in feed-forward layer\n","  Returns:\n","    A list of layers that maps (activations, mask) to (activations, mask).\n","  \"\"\"\n","  attention = tl.Attention(\n","      d_model, n_heads=n_heads, dropout=dropout, mode=mode)\n","\n","  dropout_ = tl.Dropout(\n","      rate=dropout, name='dropout_enc_attn', mode=mode)\n","\n","  feed_forward = _FeedForwardBlock(\n","      d_model, d_ff, dropout, layer_idx, mode, ff_activation)\n","\n","  return [\n","      tl.Residual(\n","          tl.LayerNorm(),\n","          attention,\n","          dropout_,\n","      ),\n","      tl.Residual(\n","          feed_forward\n","      ),\n","  ]\n","\n","@tl.layer()\n","def subsetdata(x, startidx, stopidx, **kwargs):\n","  # print(x.shape)\n","  x = x[:, startidx:stopidx, :]\n","  # print(x.shape)\n","  return x\n","\n","@gin.configurable()\n","def non_tokenizing_transformer(n_classes=57,\n","                               d_model=512, #512\n","                               d_ff=2048, #2048\n","                               n_layers=2,\n","                               n_heads=8,\n","                               dropout=0.1,\n","                               max_len=1001,\n","                               mode='train',\n","                               ff_activation=tl.Relu):\n","  \"\"\"Returns a Transformer encoder model.\n","\n","  The input to the model is a tensor of tokens.\n","\n","  Args:\n","    n_classes: how many classes on output\n","    d_model: int:  depth of embedding\n","    d_ff: int: depth of feed-forward layer\n","    n_layers: int: number of encoder/decoder layers\n","    n_heads: int: number of attention heads\n","    dropout: float: dropout rate (how much to drop out)\n","    max_len: int: maximum symbol length for positional encoding\n","    mode: str: 'train' or 'eval'\n","\n","  Returns:\n","    A Transformer model as a layer that maps from a tensor of tokens to\n","    activations over a set of output classes.\n","  \"\"\"\n","  # embedder = [\n","  #     # tl.Embedding(d_model, vocab_size),\n","  #     tl.Dense(d_model),\n","  #     tl.Dropout(rate=dropout, name='emb_dropout', mode=mode),\n","  #     tl.PositionalEncoding(max_len=max_len),\n","  # ]\n","  # return tl.Serial(                             #      tokens\n","  #    tl.Dup(),                                 # toks toks\n","  #    tl.Parallel(embedder, no_padding_mask()),  # vecs mask\n","  #    [encoder_blocks(d_model, d_ff, n_heads, dropout, i, mode)\n","  #     for i in range(n_layers)],               # vecs mask\n","  #    tl.Parallel([], tl.Drop()),               # ____  0\n","  #    tl.LayerNorm(),                           # vecs\n","  #    tl.Mean(axis=1),  # Average on length.    # vecs\n","  #    tl.Dense(n_classes)                      # vecs\n","  #    # tl.LogSoftmax()\n","  # )\n","  positional_encoder = [\n","    # tl.Embedding(d_model, vocab_size),\n","    tl.Dense(d_model),\n","    tl.Dropout(rate=dropout, name='emb_dropout', mode=mode),\n","    tl.PositionalEncoding(max_len=max_len)]\n","\n","  encoder_blocks = [\n","      tl._EncoderBlock(d_model, d_ff, n_heads, dropout, i, mode, ff_activation)\n","      for i in range(n_layers)]\n","\n","  # Assemble and return the model.\n","  return tl.Serial(                               # toks\n","      # Encode.\n","      tl.Branch(positional_encoder, no_padding_mask()),  # vecs masks\n","      encoder_blocks,                             # vecs masks\n","      tl.Select([0], n_in=2),                     # vecs\n","      tl.LayerNorm(),                             # vecs\n","      subsetdata(startidx=498, stopidx=501),\n","      # Map to output categories.\n","      tl.Mean(axis=1),                            # vecs\n","      tl.Dense(n_classes),                        # vecs\n","  )\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Le26Dr14B0J2","colab_type":"text"},"source":["# **Run**"]},{"cell_type":"markdown","metadata":{"id":"GAUh0RTYWLNj","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"kI3ihogn4eI4","colab_type":"code","outputId":"748e77b7-3ccd-4f5b-aa6e-a73e24143b83","executionInfo":{"status":"error","timestamp":1577928598941,"user_tz":480,"elapsed":544344,"user":{"displayName":"Vikram Agarwal","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB_g3ycHyXK2wQJuaywzxudHa4XAk3F1LNDGJG9=s64","userId":"13055151899643549382"}},"colab":{"height":960}},"source":["from scipy import stats\n","\n","# # Config Trax for simple synthetic data problem\n","# gin.parse_config(\"\"\"\n","# # Parameters for MultifactorSchedule:\n","# # ==============================================================================\n","# MultifactorSchedule.constant = 0.1\n","# MultifactorSchedule.factors = 'constant * linear_warmup * rsqrt_decay'\n","# MultifactorSchedule.warmup_steps = 8000\n","# \"\"\")\n","\n","timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")\n","output_dir = os.path.expanduser(\"/tmp/trax_%s\" % timestamp)\n","print(output_dir)\n","\n","_ = trainer_lib.train(model=non_tokenizing_transformer,\n","               optimizer=trax_optimizers.Adam, #Adam, #RMSProp, Adafactor\n","               loss_fn=tl.L2LossScalar, #CrossEntropyLossScalar, #L2LossScalar, #CrossEntropyLossScalar, \n","               inputs=vikram_inputs,\n","               output_dir=output_dir,\n","               steps=20000,\n","               eval_steps=10,\n","               eval_frequency=1000,\n","               has_weights=False,\n","               metrics = {'loss': trax.layers.metrics.L2Scalar})"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/tmp/trax_20200101_1720\n","/readahead/200M/cns/jq-d/home/levskaya/calico/vikram/\n","Step      0: Starting training using 8 devices\n","Step      0: Total number of trainable weights: 6907961\n","\n","Step      1: Ran 1 train steps in 39.31 secs\n","Step      1: Evaluation\n","Step      1: train loss |  136.64440765\n","Step      1: eval  loss |  143.02021942\n","Step      1: Finished evaluation\n","\n","Step   1000: Ran 999 train steps in 133.47 secs\n","Step   1000: Evaluation\n","Step   1000: train loss |  32.59833298\n","Step   1000: eval  loss |  22.95791292\n","Step   1000: Finished evaluation\n","\n","Step   2000: Ran 1000 train steps in 110.77 secs\n","Step   2000: Evaluation\n","Step   2000: train loss |  26.39411240\n","Step   2000: eval  loss |  29.57369671\n","Step   2000: Finished evaluation\n","\n","Step   3000: Ran 1000 train steps in 110.58 secs\n","Step   3000: Evaluation\n","Step   3000: train loss |  20.64959049\n","Step   3000: eval  loss |  25.49830055\n","Step   3000: Finished evaluation\n","\n","Step   4000: Ran 1000 train steps in 108.64 secs\n","Step   4000: Evaluation\n","Step   4000: train loss |  20.35082226\n","Step   4000: eval  loss |  27.14448090\n","Step   4000: Finished evaluation\n","\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m<ipython-input-75-7ca796489fc9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m                \u001b[0meval_frequency\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m                \u001b[0mhas_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m                metrics = {'loss': trax.layers.metrics.L2Scalar})\n\u001b[0m","\u001b[1;32m/export/hda3/borglet/remote_hdd_fs_dirs/0.colab_kernel_deepmind_tpu_py3_vikrama.kernel.vikrama.712809300788.14b334fb3717c109/mount/server/dm_notebook3_tpu.par/google3/third_party/py/gin/config.py\u001b[0m in \u001b[0;36mgin_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m       \u001b[0merr_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m/google_src/head/depot/google3/third_party/py/trax/supervised/trainer_lib.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(output_dir, model, loss_fn, inputs, optimizer, lr_schedule, trainer_class, steps, checkpoints_at, eval_steps, eval_frequency, random_seed, save_graphs, save_backward_graph, has_weights, nontrainable_param_map, mask_id, metrics)\u001b[0m\n\u001b[0;32m    610\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    611\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mepoch_steps\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 612\u001b[1;33m     \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    613\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m     \u001b[1;31m# Update nontrainable parameters with new history\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m/google_src/head/depot/google3/third_party/py/trax/supervised/trainer_lib.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(self, n_steps, n_eval_steps)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m       \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_stream\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    277\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_devices\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# TODO(lukaszkaiser): use everywhere if possible.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_reshape_by_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_devices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m/export/hda3/borglet/remote_hdd_fs_dirs/0.colab_kernel_deepmind_tpu_py3_vikrama.kernel.vikrama.712809300788.14b334fb3717c109/mount/server/dm_notebook3_tpu.par/google3/third_party/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3645\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3646\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3648\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m/export/hda3/borglet/remote_hdd_fs_dirs/0.colab_kernel_deepmind_tpu_py3_vikrama.kernel.vikrama.712809300788.14b334fb3717c109/mount/server/dm_notebook3_tpu.par/google3/third_party/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3641\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3642\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3643\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3645\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m/export/hda3/borglet/remote_hdd_fs_dirs/0.colab_kernel_deepmind_tpu_py3_vikrama.kernel.vikrama.712809300788.14b334fb3717c109/mount/server/dm_notebook3_tpu.par/google3/third_party/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# For Python 3 compatibility\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 631\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m/export/hda3/borglet/remote_hdd_fs_dirs/0.colab_kernel_deepmind_tpu_py3_vikrama.kernel.vikrama.712809300788.14b334fb3717c109/mount/server/dm_notebook3_tpu.par/google3/third_party/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m     \u001b[1;34m\"\"\"Returns a nested structure of `Tensor`s containing the next element.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 675\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    676\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m/export/hda3/borglet/remote_hdd_fs_dirs/0.colab_kernel_deepmind_tpu_py3_vikrama.kernel.vikrama.712809300788.14b334fb3717c109/mount/server/dm_notebook3_tpu.par/google3/third_party/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    658\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    659\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 660\u001b[1;33m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[0;32m    661\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    662\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m/export/hda3/borglet/remote_hdd_fs_dirs/0.colab_kernel_deepmind_tpu_py3_vikrama.kernel.vikrama.712809300788.14b334fb3717c109/mount/server/dm_notebook3_tpu.par/google3/third_party/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next_sync\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   2467\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"IteratorGetNextSync\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2468\u001b[0m         \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"output_types\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2469\u001b[1;33m         \"output_shapes\", output_shapes)\n\u001b[0m\u001b[0;32m   2470\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2471\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"Ieq7f_qfgYfJ","colab_type":"text"},"source":["# **Test pretrained NN model**"]},{"cell_type":"code","metadata":{"id":"da1fD1Y8gWAh","colab_type":"code","outputId":"51d863cd-8f3d-42c0-e737-b76d5b64d4b2","executionInfo":{"status":"ok","timestamp":1577930154877,"user_tz":480,"elapsed":75619,"user":{"displayName":"Vikram Agarwal","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB_g3ycHyXK2wQJuaywzxudHa4XAk3F1LNDGJG9=s64","userId":"13055151899643549382"}},"colab":{"height":367}},"source":["#print(backend.device_count())\n","# output_dir='/tmp/trax_20191125_1349'\n","\n","# Inference model\n","predict_model = non_tokenizing_transformer(mode='eval')\n","predict_signature = shapes.ShapeDtype((1, 1001, 117), dtype=np.float32) #shape of input\n","# predict_signature = shapes.ShapeDtype((1, 2, 117), dtype=np.float32) #shape of input\n","predict_model.init(predict_signature)\n","\n","# Load from file (API for trainer_state may change)\n","trainer_state = trainer_lib.load_trainer_state(output_dir)\n","# predict_model.params = trainer_state.opt_state.weights[0]\n","\n","weights = trainer_state.opt_state.weights[0]\n","model_state = trainer_state.model_state[0]\n","rng = backend.random.get_prng(0)\n","\n","# Run inference\n","preds = []\n","obs = []\n","ds_test = vikram_inputs(mode='test', n_batch=2)\n","\n","# make dataset iterators\n","for input in ds_test:\n","  # print(input[0].shape)\n","  preds.append(predict_model(input[0], weights=weights, state=model_state, rng=rng)) #, rng=random_key\n","  obs.append(np.squeeze(input[1]))\n","preds = np.vstack(preds)\n","obs = np.vstack(obs)\n","\n","print(preds.shape)\n","print(obs.shape)\n","print(preds.dtype)\n","print(obs.dtype)\n","print(preds)\n","\n","#preds = [1 if x < 0.5 else 0 for x in np.exp(preds[:,0])] \n","#print(preds)\n","\n","# import matplotlib.pyplot as plt\n","# fig1, ax1 = plt.subplots()\n","# ax1.hist(np.exp(preds[:,0]), bins=100)\n","# fig1, ax1 = plt.subplots()\n","# ax1.hist(obs[:,0], bins=100)\n","\n","# from sklearn import metrics\n","# print(metrics.accuracy_score(obs[:,0], preds[:,0]))\n","\n","slope, intercept, r_value, p_value, std_err = stats.mstats.linregress(preds[:,53], obs[:,53])\n","print('Test R^2 = %.3f' % (r_value**2))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model loaded from /tmp/trax_20200101_1720/model.pkl at step 4000\n","/readahead/200M/cns/jq-d/home/levskaya/calico/vikram/\n","(1000, 57)\n","(1000, 57)\n","float32\n","float32\n","[[-1.1813539  -0.6862472  -0.86021227 ... -0.998313   -1.2930957\n","  -0.81592447]\n"," [ 0.11105715  0.25464073  0.3180077  ... -0.40874064 -0.08990811\n","   0.61788934]\n"," [ 0.27884412  0.20377345  0.2841013  ...  0.31727543  0.36500064\n","   0.64982074]\n"," ...\n"," [ 0.33665946  0.13464828  0.23719117 ...  0.49370202  0.33674708\n","   0.64944476]\n"," [-1.0406121  -0.94504756 -0.9263856  ... -0.71154547 -0.85463125\n","  -0.9171174 ]\n"," [-1.273698   -1.0722353  -1.052209   ... -0.87417805 -0.88557506\n","  -0.91105646]]\n","Test R^2 = 0.615\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"c23msqmqWPRF","colab_type":"text"},"source":["# Baseline Linear Reg"]},{"cell_type":"code","metadata":{"id":"mJfZysb2Whpc","colab_type":"code","outputId":"31253c28-bfc2-4755-c73c-05c260dba6f2","executionInfo":{"status":"ok","timestamp":1577930024227,"user_tz":480,"elapsed":236670,"user":{"displayName":"Vikram Agarwal","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB_g3ycHyXK2wQJuaywzxudHa4XAk3F1LNDGJG9=s64","userId":"13055151899643549382"}},"colab":{"height":332}},"source":["import numpy as np\n","from scipy import stats\n","from sklearn import linear_model, metrics\n","\n","obs = []\n","inp = []\n","ds_train = vikram_inputs(mode='linregtrain', n_batch=2000)\n","\n","###TRAINING\n","\n","# make dataset iterators\n","for input in ds_train:\n","  print(input[0].shape)\n","  inp.append(np.mean(input[0], axis=1))\n","  obs.append(np.squeeze(input[1]))\n","\n","obs = np.vstack(obs)\n","inp = np.vstack(inp)\n","\n","print(obs.shape)\n","print(inp.shape)\n","\n","regr = linear_model.LinearRegression()\n","# regr = linear_model.LogisticRegression()\n","regr.fit(inp, obs[:,1])\n","\n","\n","###TESTING\n","\n","obs = []\n","inp = []\n","ds_test = vikram_inputs(mode='test', n_batch=200)\n","\n","# make dataset iterators\n","for input in ds_test:\n","  print(input[0].shape)\n","  inp.append(np.mean(input[0], axis=1))\n","  obs.append(np.squeeze(input[1]))\n","\n","obs = np.vstack(obs)\n","inp = np.vstack(inp)\n","y_hat = regr.predict(inp)\n","\n","# print(metrics.accuracy_score(obs[:,0], y_hat))\n","slope, intercept, r_value, p_value, std_err = stats.linregress(obs[:,53], y_hat)\n","print('BASELINE: Test R^2 = %.3f' % r_value**2)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/readahead/200M/cns/jq-d/home/levskaya/calico/vikram/\n","(2000, 2, 117)\n","(2000, 2, 117)\n","(2000, 2, 117)\n","(2000, 2, 117)\n","(2000, 2, 117)\n","(2000, 2, 117)\n","(2000, 2, 117)\n","(1877, 2, 117)\n","(15877, 57)\n","(15877, 117)\n","/readahead/200M/cns/jq-d/home/levskaya/calico/vikram/\n","(200, 2, 117)\n","(200, 2, 117)\n","(200, 2, 117)\n","(200, 2, 117)\n","(200, 2, 117)\n","BASELINE: Test R^2 = 0.524\n"],"name":"stdout"}]}]}