import google3.experimental.users.vikrama.vikramtrax
# Parameters for inputs:
# ==============================================================================
vikram_inputs.cns_path = '/readahead/200M/cns/tm-d/home/vikrama/'
vikram_inputs.n_batch = 2048
#vikram_inputs.dataset_name = 't2t_expression_level_predict_pca'
#inputs.input_name = 'inputs'
#inputs.append_targets = True
# Parameters for MultifactorSchedule:
# ==============================================================================
MultifactorSchedule.constant = 1
MultifactorSchedule.factors = 'constant * linear_warmup * rsqrt_decay'
MultifactorSchedule.warmup_steps = 8000
# Parameters for train:
# ==============================================================================
train.eval_frequency = 1000
train.eval_steps = 10
train.inputs = @vikramtrax.vikram_inputs
train.model = @vikramtrax.non_tokenizing_transformer
train.train_steps = 10000 #60000
train.optimizer = @trax.optimizers.Adam
train.loss_fn = @L2LossScalar
train.metrics = { "loss": @L2LossScalar }
# Parameters for non_tokenizing_transformer:
# ==============================================================================
non_tokenizing_transformer.d_model = 512 #divides by heads, keep >=n_heads*128
non_tokenizing_transformer.d_ff = 2048
non_tokenizing_transformer.dropout = 0.1
non_tokenizing_transformer.max_len = 1001
non_tokenizing_transformer.mode = 'train'
non_tokenizing_transformer.n_classes = 57
non_tokenizing_transformer.n_heads = 2 #try 4 & 8
non_tokenizing_transformer.n_layers = 2
